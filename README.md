# Awesome-Visual-Reasoning

üëÅÔ∏è This is a repository for organizing recent papers on visual reasoning. 

üìö *This repository organizes papers on **visual reasoning**, from CLEVR to the latest works, with a particular focus on **multimodal large language models (MLLMs)**.*

---

## üìÑ Papers

### Survey:

Vqa and visual reasoning: An overview of recent datasets, methods and challenges (2022)„ÄÅ

Efficient Reasoning Models: A SurveyÔºà2025Ôºâ

A Survey of Reasoning with Foundation ModelsÔºà2025Ôºâ

A Survey of Reasoning with Foundation Models: Concepts, Methodologies, and OutlookÔºà2023Ôºâ

The Revolution of Multimodal Large Language Models: A SurveyÔºà2024Ôºâ

Multimodal Chain-of-Thought Reasoning: A Comprehensive SurveyÔºà2025.03Ôºâ

Towards reasoning era: A survey of long chain-of-thought for reasoning large language modelsÔºà2025.03Ôºâ

Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers Ôºà2025.06Ôºâ

### 2015 - 2017

- [Vqa: Visual question answering](http://openaccess.thecvf.com/content_iccv_2015/html/Antol_VQA_Visual_Question_ICCV_2015_paper.html) [ICCV 2015]
- Neural Module Networks.(NMN)[CVPR2016]
- Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding (MCB)[ECCV2016]
- Stacked Attention Networks for Image Question Answering (SAN)[CVPR2016]
- Hierarchical Question-Image Co-Attention for Visual Question Answering [NIPS2016]
- Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering[CVPR2018]

### 2017 - 2019

**CLEVR: Representative Methods**

- **CLEVR**: A diagnostic dataset for compositional language and elementary visual reasoning.[CVPR 2017]

- Inferring and Executing Programs for Visual Reasoning. ÔºàCLEVRÔºâ[ICCV2017]
- A simple neural network module for relational reasoning.ÔºàCLEVRÔºâ[NIPS 2017]
- Learning to reason: End-to-end module networks for visual question answering. (CLEVR + VQA)[ICCV 2017]
- FiLM: Visual Reasoning with a General Conditioning Layer. (CLEVR) [AAAI2018]
- Compositional Attention Networks for Machine Reasoning. (MAC)[ICLR2018]
- Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding.[NIPS2018]
- Bilinear Attention Networks(BAN) [NIPS2018]

### 2019-2021

**GQA / VCR: Representative Methods**Ôºö

- **Gqa:** A new dataset for real-world visual reasoning and compositional question answering.[CVPR 2019]
- From recognition to cognition: Visual commonsense reasoningÔºàVCRÔºâ[CVPR 2019]
- VilBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks[NIPS2019] (Ê†áÂøóÁùÄÊé®ÁêÜÂü∫Êú¨ËøõÂÖ•pre-trained Êó∂‰ª£)
- LXMERT: Learning Cross-Modality Encoder Representations from Transformers[EMNLP2019]
- UNITER: Universal Image-Text Representation Learning[ECCV2020]
- Align before Fuse: Vision and Language Representation Learning with Momentum Distillation[NIPS2021]
- ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph[AAAI2021]

### 2022Ëá≥‰ªä

Visual spatial reasoning [TACL2023]

Flamingo: a visual language model for few-shot learning[NIPS2022] (ÊúÄÊó©ÂºÄÂßã)

STaR: Bootstrapping Reasoning With Reasoning [NIPS2022]

BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models[ICML2023]

InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning[NIPS2023]

Visual Instruction Tuning (LLaVA) [NIPS2023]

Improved Baselines with Visual Instruction Tuning (LLaVA-1.5)[2023]

Visual chatgpt: Talking, drawing and editing with visual foundation models.[2023.03]

V‚àó: Guided Visual Search as a Core Mechanism in Multimodal LLMs [CVPR2024]

Spatialvlm: Endowing vision-language models with spatial reasoning capabilities[CVPR2024]

LLaVA-NeXT: Stronger Large Multimodal Models with Better Training and Data [2024]

Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond[2023]

Qwen2-VL: Enhancing Vision-Language Models with Multi-granularity Alignment[2024]

Take A Step Back: Rethinking the Two Stages in Visual Reasoning[ECCV2024]

TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation ModelsÔºà2024.10Ôºâ

Visual SKETCHPAD: Sketching as a Visual Chain of Thought for Multimodal Language ModelsÔºà2024.11Ôºâ

Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree searchÔºà2024.12Ôºâ

Taco: Learning multi-modal action models with synthetic chains-of-thought-and-actionÔºà2024.12Ôºâ

Imagine while Reasoning in Space: Multimodal Visualization-of-ThoughtÔºà2025.01Ôºâ

LIMO: Less is More for ReasoningÔºà2025.02Ôºâ

Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rlÔºà2025.03Ôºâ

Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learningÔºà2025.03Ôºâ

VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language ModelsÔºà2025.04Ôºâ

Thinking with Generated ImagesÔºà2025.05Ôºâ

Visual Planning: Let's Think Only with ImagesÔºà2025.05Ôºâ

DeepEyes: Incentivizing "Thinking with Images" via Reinforcement LearningÔºà2025.05Ôºâ

Reasoning Models Don‚Äôt Always Say What They ThinkÔºà2025.05Ôºâ

Look-Back: Implicit Visual Re-focusing in MLLM ReasoningÔºà2025.07Ôºâ

Learning Only with Images: Visual Reinforcement Learning with Reasoning, Rendering, and Visual FeedbackÔºà2025.07Ôºâ



